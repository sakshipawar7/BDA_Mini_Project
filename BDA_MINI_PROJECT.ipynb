{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Locally installing Spark"
      ],
      "metadata": {
        "id": "0T5K6O4dXcmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_twJIIcR50s",
        "outputId": "db9e05bd-1b84-44a2-a86d-b57b64a8f23f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master('local[*]') \\\n",
        "        .appName('Basics') \\\n",
        "        .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:"
      ],
      "metadata": {
        "id": "If3nOAZHXhgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MatrixVectorMultiplication\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Input matrix and vector\n",
        "matrix = [\n",
        "    [7, 8, 9],\n",
        "    [4, 5, 6],\n",
        "    [10, 11, 12]\n",
        "]\n",
        "\n",
        "vector = [1, 2, 3]\n",
        "\n",
        "# Define the multiplication function\n",
        "def multiply(row):\n",
        "    matrix_row, values = row\n",
        "    result = sum(value * vector[i] for i, value in enumerate(values))\n",
        "    return (matrix_row, result)\n",
        "\n",
        "# Parallelize the matrix\n",
        "matrix_rdd = spark.sparkContext.parallelize(enumerate(matrix))\n",
        "\n",
        "# Perform matrix-vector multiplication\n",
        "result = matrix_rdd.map(multiply)\n",
        "\n",
        "# Collect the result and print\n",
        "print(result.collect())\n",
        "\n",
        "# Stop the Spark Session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KJcYimETXJf",
        "outputId": "2fc0af14-2fc7-4a27-a99b-35e137d56778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 50), (1, 32), (2, 68)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:"
      ],
      "metadata": {
        "id": "Wepo_d6yXkLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import sqrt\n",
        "\n",
        "# Dummy input data\n",
        "input_data = [\n",
        "    'key1\\t10',\n",
        "    'key2\\t20',\n",
        "    'key1\\t30',\n",
        "    'key2\\t40',\n",
        "    'key1\\t50',\n",
        "    'key2\\t60',\n",
        "]\n",
        "\n",
        "def map_func(line):\n",
        "    key, value = line.split('\\t')\n",
        "    return key, float(value)\n",
        "\n",
        "def reduce_func(data):\n",
        "    values = list(data)  # Convert data to list for clarity\n",
        "    mean_val = sum(values) / len(values)\n",
        "    sum_val = sum(values)\n",
        "    if len(values) > 1:  # Check if there are more than one value for calculation\n",
        "        std_dev_val = sqrt(sum((x - mean_val) ** 2 for x in values) / (len(values) - 1))\n",
        "    else:\n",
        "        std_dev_val = 0\n",
        "    return {\n",
        "        'mean': mean_val,\n",
        "        'sum': sum_val,\n",
        "        'std_dev': std_dev_val\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sc = SparkContext('local', 'AggregationSpark')\n",
        "    try:\n",
        "        lines = sc.parallelize(input_data)\n",
        "        mapped = lines.map(map_func)\n",
        "        grouped = mapped.groupByKey()\n",
        "        result = grouped.mapValues(list).mapValues(reduce_func)\n",
        "        output = result.collect()\n",
        "        for key, value in output:\n",
        "            print(f'{key}\\t{value}')\n",
        "    finally:\n",
        "        sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDub3aF9URyz",
        "outputId": "a3581a1b-a357-4e9b-df4c-4404c2e116a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key1\t{'mean': 30.0, 'sum': 90.0, 'std_dev': 20.0}\n",
            "key2\t{'mean': 40.0, 'sum': 120.0, 'std_dev': 20.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:"
      ],
      "metadata": {
        "id": "8LR4plM5Xmf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SortData\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define dummy input data\n",
        "dummy_data = [\n",
        "    \"3\\tTable\",\n",
        "    \"1\\tChair\",\n",
        "    \"2\\tDesk\",\n",
        "    \"4\\tWindows\"\n",
        "]\n",
        "\n",
        "# Create RDD from dummy data\n",
        "data_rdd = spark.sparkContext.parallelize(dummy_data)\n",
        "\n",
        "# Sort the data based on the first column\n",
        "sorted_data = data_rdd.sortBy(lambda x: x.split('\\t')[0])\n",
        "\n",
        "# Collect and print the sorted data\n",
        "sorted_results = sorted_data.collect()\n",
        "for result in sorted_results:\n",
        "    print(result)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmcVxSlOW3bJ",
        "outputId": "ff9a08d4-ccbc-4ec5-b5fd-b662ebcc2a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\tChair\n",
            "2\tDesk\n",
            "3\tTable\n",
            "4\tWindows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:"
      ],
      "metadata": {
        "id": "vk8uqZgXXonK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a Spark context\n",
        "conf = SparkConf().setAppName(\"SearchElement\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Define the data to be searched\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Parallelize the data into RDD (Resilient Distributed Dataset)\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Define the search function\n",
        "def search_element(element):\n",
        "    return element == 10  # Change the search element as needed\n",
        "\n",
        "# Map function to search for the element in the dataset\n",
        "result = rdd.map(search_element)\n",
        "\n",
        "# Collect the results\n",
        "search_result = result.collect()\n",
        "\n",
        "# Print the search result\n",
        "if True in search_result:\n",
        "    print(\"Element found in the dataset\")\n",
        "else:\n",
        "    print(\"Element not found in the dataset\")\n",
        "\n",
        "# Stop the Spark context\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJjy2ya8W6Ik",
        "outputId": "86ca5440-453e-4147-8c48-7e5f3d2fefc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element found in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5:"
      ],
      "metadata": {
        "id": "arorRCqqXqgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Joins\")\n",
        "\n",
        "# Create RDDs for left and right datasets\n",
        "left_data = sc.parallelize([(1, \"A\"), (2, \"B\"), (3, \"C\")])\n",
        "right_data = sc.parallelize([(1, \"P\"), (3, \"Q\"), (4, \"R\")])\n",
        "\n",
        "# Perform map-side join\n",
        "map_join = left_data.join(right_data)\n",
        "\n",
        "# Perform reduce-side join\n",
        "reduce_join = left_data.union(right_data).reduceByKey(lambda x, y: (x, y))\n",
        "\n",
        "# Print the results\n",
        "print(\"Map Side Join:\", map_join.collect())\n",
        "print(\"Reduce Side Join:\", reduce_join.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6rNQjCXZ0h",
        "outputId": "e6a6af0c-ac1c-40fe-9d10-a5cb4bb2afb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map Side Join: [(1, ('A', 'P')), (3, ('C', 'Q'))]\n",
            "Reduce Side Join: [(2, 'B'), (4, 'R'), (1, ('A', 'P')), (3, ('C', 'Q'))]\n"
          ]
        }
      ]
    }
  ]
}